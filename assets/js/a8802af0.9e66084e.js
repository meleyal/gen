"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[753],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return h}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),m=d(n),h=i,u=m["".concat(l,".").concat(h)]||m[h]||p[h]||o;return n?a.createElement(u,r(r({ref:t},c),{},{components:n})):a.createElement(u,r({ref:t},c))}));function h(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,r=new Array(o);r[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,r[1]=s;for(var d=2;d<o;d++)r[d]=n[d];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3062:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return d},toc:function(){return c},default:function(){return m}});var a=n(3117),i=n(102),o=(n(7294),n(3905)),r=["components"],s={title:"JavaScript"},l=void 0,d={unversionedId:"primers/javascript",id:"primers/javascript",title:"JavaScript",description:"In this chapter we'll look at the features available in JavaScript for working",source:"@site/docs/primers/javascript.md",sourceDirName:"primers",slug:"/primers/javascript",permalink:"/generative-music-with-javascript/primers/javascript",tags:[],version:"current",frontMatter:{title:"JavaScript"},sidebar:"main",previous:{title:"Music",permalink:"/generative-music-with-javascript/primers/music"},next:{title:"Notes",permalink:"/generative-music-with-javascript/music/notes"}},c=[{value:"Background",id:"background",children:[],level:2},{value:"The Web Audio API",id:"the-web-audio-api",children:[],level:2},{value:"Graphs &amp; Nodes",id:"graphs--nodes",children:[{value:"Creating Our Graph",id:"creating-our-graph",children:[],level:3}],level:2},{value:"Nodes",id:"nodes",children:[{value:"Source Nodes",id:"source-nodes",children:[{value:"OscillatorNode",id:"oscillatornode",children:[],level:4},{value:"AudioBufferSourceNode",id:"audiobuffersourcenode",children:[],level:4}],level:3},{value:"Effect Nodes",id:"effect-nodes",children:[],level:3},{value:"Destination Node",id:"destination-node",children:[],level:3}],level:2},{value:"Timing Model",id:"timing-model",children:[{value:"JavaScript Timing",id:"javascript-timing",children:[],level:3},{value:"Web Audio API Timing",id:"web-audio-api-timing",children:[],level:3}],level:2},{value:"Aside: Autoplay Policy",id:"aside-autoplay-policy",children:[],level:2},{value:"Conclusion",id:"conclusion",children:[],level:2},{value:"Further Reading",id:"further-reading",children:[],level:2}],p={toc:c};function m(e){var t=e.components,s=(0,i.Z)(e,r);return(0,o.kt)("wrapper",(0,a.Z)({},p,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"In this chapter we'll look at the features available in JavaScript for working\nwith audio, namely the Web Audio API. Technically, the Web Audio API is not part\nof the JavaScript language itself, but is a standard part of the web platform\nimplemented by modern browsers."),(0,o.kt)("p",null,"We won't cover JavaScript itself here as it's already well documented elsewhere.\nIf it's your first time using the language or you need a refresher, refer to the\n",(0,o.kt)("a",{parentName:"p",href:"#further-reading"},"Further Reading")," section at the end of this chapter for some\nrecommendations."),(0,o.kt)("p",null,"We'll start by defining what the Web Audio API is and where it came from, then\nmove on to cover two of it's key concepts: graphs and timing. Along the way\nwe'll get a feel for the API and what it can do."),(0,o.kt)("h2",{id:"background"},"Background"),(0,o.kt)("p",null,"The Web Audio API is a set of APIs for generating and processing audio in the\nbrowser. It's designed by the\n",(0,o.kt)("a",{parentName:"p",href:"https://www.w3.org/2011/audio/"},"W3C Audio Working Group"),' and chartered to "add\nadvanced sound and music synthesis capabilities to the Open Web Platform."'),(0,o.kt)("p",null,"It has been in development\n",(0,o.kt)("a",{parentName:"p",href:"https://www.w3.org/TR/2011/WD-webaudio-20111215/"},"since 2011"),", with the spec\nrecently reaching v1.0 (with\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/WebAudio/web-audio-api-v2"},"v2.0 in the works"),") and becoming\na ",(0,o.kt)("a",{parentName:"p",href:"https://www.w3.org/TR/webaudio/"},"W3C Candidate Recomendation"),". Despite its\ncandiditate status, it's already well supported in browsers,\n",(0,o.kt)("a",{parentName:"p",href:"https://caniuse.com/#feat=audio-api"},"reaching 96% of users")," at time of writing."),(0,o.kt)("p",null,"Common use-cases cited for the Web Audio API are to bring music composition and\naudio editing tools (e.g. Ableton Live, Logic Pro) to the browser, as well as\nenabling real-time audio for games and VR. Beyond these obvious applications,\nbringing sound to the mix opens up another dimension for building a more sensory\nweb."),(0,o.kt)("h2",{id:"the-web-audio-api"},"The Web Audio API"),(0,o.kt)("p",null,"The Web Audio API itself is relatively small, and is covered comprehensively by\nthe\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API"},"MDN Web Docs"),",\nthe go-to source for understanding everything the API can do."),(0,o.kt)("p",null,"Rather than repeat the documentation here, we'll instead focus on the two\naspects of working with the API that are most different from traditional web\ndevelopment, namely the audio graph and the timing model. We'll cover just\nenough of the API to get up and running creating our own bleeps and bloops."),(0,o.kt)("h2",{id:"graphs--nodes"},"Graphs & Nodes"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"RETHINK")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Trees are not their own thing, they are a type of graph."))),(0,o.kt)("p",null,"As web developers, we're used to working with the DOM, a tree data structure\nrepresenting a hierarchy of nodes that we traverse via parent, sibling and child\nrelationships. The Web Audio API, on the other hand (and audio apps in general),\norganizes nodes in a ",(0,o.kt)("em",{parentName:"p"},"graph")," data structure."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Tree vs. graph"))),(0,o.kt)("p",null,(0,o.kt)("em",{parentName:"p"},"Source")," nodes generate signals and are the inputs of our system. These signals\nare routed through ",(0,o.kt)("em",{parentName:"p"},"effect")," nodes to modify them in various ways. Everything\nends up at a single ",(0,o.kt)("em",{parentName:"p"},"destination")," node, i.e. our speakers, producing the audible\noutput of our system. This is digital signal processing 101."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Graph"))),(0,o.kt)("p",null,"It's worth taking a moment to understand the fundamental differences between\ntree and graph data structures.\n",(0,o.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Graph_(abstract_data_type)"},"Graph theory"),' is\nit\'s own rich topic, but a key point to note is that a graph is not a heirarchy,\nbut is instead like a flow chart or electrical circuit. Each node is "equal" and\ncan be connected to any (and many) other nodes, and connections can be circular\n(in fact this is essential to produce certain types of effects). Specifically,\nthe Web Audio API uses a ',(0,o.kt)("em",{parentName:"p"},"directed graph"),", that is, signals flow in a single\ndirection, from source to destination."),(0,o.kt)("p",null,"If we think about it, this is not such a new concept. We build pages (nodes)\nwhich we link together to form websites and apps (graphs), which together form\nthe internet, itself a graph of servers, routers, etc."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Internet network"))),(0,o.kt)("p",null,"A big part of working with the Web Audio API involves creating nodes and making\nsure they are wired up correctly in our graph. So let's see how this works..."),(0,o.kt)("h3",{id:"creating-our-graph"},"Creating Our Graph"),(0,o.kt)("p",null,"Our graph exists in an ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext"),". Everything we do via the Web Audio API\nhappens in this context, similar to how a ",(0,o.kt)("inlineCode",{parentName:"p"},"<canvas>")," element creates its own\nenvironment for drawing. We'll generally only create a single ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext")," per\napp and use its factory methods to create the nodes of our graph:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nlet osc = context.createOscillator()\nlet vol = context.createGain()\nosc.connect(vol)\nvol.connect(context.destination)\n")),(0,o.kt)("p",null,"Here we create an ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext"),", and from it create two different node types.\nan ",(0,o.kt)("inlineCode",{parentName:"p"},"OscillatorNode"),", a type of ",(0,o.kt)("em",{parentName:"p"},"source")," node, and a ",(0,o.kt)("inlineCode",{parentName:"p"},"GainNode"),", a type of\n",(0,o.kt)("em",{parentName:"p"},"effect")," node (we'll cover what these nodes actually do shortly). We connect the\n",(0,o.kt)("inlineCode",{parentName:"p"},"OscillatorNode")," to the ",(0,o.kt)("inlineCode",{parentName:"p"},"GainNode"),", and the ",(0,o.kt)("inlineCode",{parentName:"p"},"GainNode")," to the ",(0,o.kt)("em",{parentName:"p"},"destination")," node\nof the ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext")," i.e. our speakers. Our graph looks as follows (rendered in\nChrome with the ",(0,o.kt)("a",{parentName:"p",href:"https://google.github.io/audion/"},"Web Audio Inspector"),")."),(0,o.kt)("p",null,(0,o.kt)("img",{src:n(4508).Z,width:"2020",height:"478"})),(0,o.kt)("h2",{id:"nodes"},"Nodes"),(0,o.kt)("p",null,"Everything we create in our graph is a node. All nodes implement the\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioNode"},(0,o.kt)("inlineCode",{parentName:"a"},"AudioNode")),"\ninterface, extended with additional properties and methods specific to their\ntype."),(0,o.kt)("p",null,"A node's properties can be get and set as you'd expect, but with an additional\nsuper-power: they implement the\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioParam"},(0,o.kt)("inlineCode",{parentName:"a"},"AudioParam")),"\ninterface, meaning changes to them can be scheduled over time (",(0,o.kt)("inlineCode",{parentName:"p"},"currentTime")," is\ncovered in more detail in the next section):"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nlet osc = context.createOscillator()\n\n// Set frequency now\nosc.frequency = 440\n\n// Change frequency in 1 second\noscillator.frequency.setValueAtTime(880, context.currentTime + 1)\n")),(0,o.kt)("p",null,"All nodes implement the ",(0,o.kt)("inlineCode",{parentName:"p"},"connect()")," method, which is how you connect the output\nof one node to the input of others. This chaining is what creates our graph."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Connected nodes showing signal flow"))),(0,o.kt)("p",null,"Nodes themselves can be grouped into two types: ",(0,o.kt)("em",{parentName:"p"},"source nodes")," and ",(0,o.kt)("em",{parentName:"p"},"effect\nnodes"),"."),(0,o.kt)("h3",{id:"source-nodes"},"Source Nodes"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},(0,o.kt)("strong",{parentName:"p"},"Source")," > Effect > Destination"))),(0,o.kt)("p",null,"Source nodes are anything that produce an audio signal and are the inputs of our\nsystem. There are several types of source node, but we'll cover just the two\nmost common here:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode"},(0,o.kt)("inlineCode",{parentName:"a"},"OscillatorNode")),"\n\u2013\xa0for synthesizing our own sounds."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode"},(0,o.kt)("inlineCode",{parentName:"a"},"AudioBufferSourceNode")),"\n\u2013 for playing back recorded sounds (samples).")),(0,o.kt)("p",null,"All source nodes have a ",(0,o.kt)("inlineCode",{parentName:"p"},"start()")," and ",(0,o.kt)("inlineCode",{parentName:"p"},"stop()")," method, which as you might guess,\nstart and stop them producing their audio signal."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"Source nodes are single use only")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},'An important aspect of source nodes is that they are single use only, or "fire\nand forget". Once a node has stopped (either by manually calling ',(0,o.kt)("inlineCode",{parentName:"p"},"stop()"),", or by\nreaching the end of the sample it was playing), it cannot be restarted."),(0,o.kt)("p",{parentName:"div"},"In creating a piano instrument, our instinct might be to create 88 nodes, one\nfor each key. Instead, we actually need to create a new node each time a key is\npressed. In this way, our audio graph is not something fixed that we define\nahead of time, but is instead a dynamic structure that changes as new nodes are\ncreated and discarded. Source nodes are intentionally cheap to create and\nstopped nodes are automatically garbage-collected for us."))),(0,o.kt)("h4",{id:"oscillatornode"},"OscillatorNode"),(0,o.kt)("p",null,"To synthesize our own sounds we can use an ",(0,o.kt)("inlineCode",{parentName:"p"},"OscillatorNode"),". This produces a\nwaveform (sine, square, triangle, etc.) oscillating at a given frequency\n(specified in hertz). By combining different types of oscillators and effects,\nwe can produce an infinite range of sounds."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\n\nconst bass = context.createOscillator()\nbass.type = 'sine'\nbass.frequency = 220\nbass.frequency.linearRampToValueAtTime(880, context.currentTime + 2)\n\nconst hi = context.createOscillator()\nhi.type = 'square'\nhi.frequency = 660\nhi.frequency.linearRampToValueAtTime(880, context.currentTime + 6)\n\nbass.connect(context.destination)\nhi.connect(context.destination)\n\nbass.start()\nhi.start()\n")),(0,o.kt)("h4",{id:"audiobuffersourcenode"},"AudioBufferSourceNode"),(0,o.kt)("p",null,"To play back recorded sounds (samples) we can use an ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioBufferSourceNode"),".\nThis node is responsible for playing back and controlling the sample, but the\nsample itself is stored in an ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioBuffer"),". In this way we can load a sample\nonce, and use it many times."),(0,o.kt)("p",null,"Loading a sample is a two step process. We first need to fetch it over the\nnetwork, then decode it into a format that ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioBuffer")," understands."),(0,o.kt)("p",null,"The whole process of loading, decoding, and finally playing back a sample looks\nas follows:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\n\nconst response = await fetch('http://example.com/meow.mp3')\nconst arrayBuffer = await response.arrayBuffer()\nconst audioBuffer = await context.decodeAudioData(arrayBuffer)\n\nconst sourceNode = context.createBufferSource()\nsourceNode.buffer = audioBuffer\nsourceNode.connect(context.destination)\nsourceNode.start()\n")),(0,o.kt)("h3",{id:"effect-nodes"},"Effect Nodes"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Source > ",(0,o.kt)("strong",{parentName:"p"},"Effect")," > Destination"))),(0,o.kt)("p",null,"We can route an audio signal (coming from a source node) through a wide range of\neffect nodes. These modify the incoming signal in some way, producing a new\nsignal as output. The Web Audio API defines some basic primitives, which can be\ncombined to create all sorts of effects. The most common ones are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/GainNode"},(0,o.kt)("inlineCode",{parentName:"a"},"GainNode"))," \u2013\nadjusts the volume of a signal. By applying this over time we can also model\nADSR envelopes (e.g. if a sound should start abruptly or fade in slowly)."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/BiquadFilterNode"},(0,o.kt)("inlineCode",{parentName:"a"},"BiquadFilterNode")),"\n\u2013 cut or boost certain frequencies of a signal"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/ConvolverNode"},(0,o.kt)("inlineCode",{parentName:"a"},"ConvolverNode")),"\n\u2013 apply reverb to a signal so it sounds like it's in a certain physical space\n(e.g. a small room or large hall)."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/DelayNode"},(0,o.kt)("inlineCode",{parentName:"a"},"DelayNode"))," -\napply a delay to the outgoing signal, used for all sorts of effects from\nechoes to phasing."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/DynamicsCompressorNode"},(0,o.kt)("inlineCode",{parentName:"a"},"DynamicsCompressorNode")),"\n\u2013 applies compression to a signal to control its dynamic range (e.g. to avoid\ndistortion)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/WaveShaperNode"},(0,o.kt)("inlineCode",{parentName:"a"},"WaveShaperNode")),"\n\u2013 applies distortion to the signal."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/PannerNode"},(0,o.kt)("inlineCode",{parentName:"a"},"PannerNode"))," \u2013\nplaces the audio in the stereo field (i.e. to the left or right in stereo\noutput).")),(0,o.kt)("h3",{id:"destination-node"},"Destination Node"),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Source > Effect > ",(0,o.kt)("strong",{parentName:"p"},"Destination")))),(0,o.kt)("p",null,"The\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioDestinationNode"},(0,o.kt)("inlineCode",{parentName:"a"},"AudioDestinationNode")),"\nis the final node in the chain and represents our audio output device (i.e. our\nsound card). This is provided for us as the ",(0,o.kt)("inlineCode",{parentName:"p"},"destination")," node of our\n",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext"),". In addition, there's also an\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/OfflineAudioContext"},(0,o.kt)("inlineCode",{parentName:"a"},"OfflineAudioContext")),"\nwith its own ",(0,o.kt)("inlineCode",{parentName:"p"},"destination")," if we want to render our audio to disk, for example."),(0,o.kt)("h2",{id:"timing-model"},"Timing Model"),(0,o.kt)("p",null,"Timing, as we'll see, is probably the trickiest aspect of working with audio.\nUnderstanding how the Web Audio API handles timing, and how this differs from\ntraditional JavaScript timing, will be essential for writing our own scheduling\nfunctions later in the book."),(0,o.kt)("h3",{id:"javascript-timing"},"JavaScript Timing"),(0,o.kt)("p",null,"In regular JavaScript, we can think of the browser as running our code in a\nsingle process. When we schedule something to happen in two seconds (using\n",(0,o.kt)("inlineCode",{parentName:"p"},"setTimeout")," or ",(0,o.kt)("inlineCode",{parentName:"p"},"setInterval"),"), the browser doesn't guarantee that it will\nhappen in ",(0,o.kt)("em",{parentName:"p"},"exactly")," two seconds, rather that it will run in ",(0,o.kt)("em",{parentName:"p"},"about")," two seconds,\ndepending on the priority of other tasks in the queue."),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},(0,o.kt)("inlineCode",{parentName:"p"},"setInterval")," example and JS fuzzy timing diagram"))),(0,o.kt)("h3",{id:"web-audio-api-timing"},"Web Audio API Timing"),(0,o.kt)("p",null,"JavaScript's loose timing model is fine (and usually desirable) for most\napplications, where millisecond delays in e.g. updating the UI would be\nimperceptable. But for music, any imprecision is immediately obvious and tends\nto get worse over time as things drift out of sync."),(0,o.kt)("p",null,'For this reason, the Web Audio API has it\'s own internal clock. Under the hood,\nour calls to the Web Audio API are sent to a seperate "rendering" thread that\nperforms the audio computation and returns the results to the main "control"\nthread (i.e. our code). It\'s in this rendering thread that the Web Audio API\nclock runs, and is the reason it can maintain highly accurate time, regardless\nof the work being performed in the main thread.'),(0,o.kt)("p",null,"We can access the Web Audio API clock via the ",(0,o.kt)("inlineCode",{parentName:"p"},"currentTime")," attribute of the\n",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\ncontext.currentTime // => 0.1234\n")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"currentTime")," starts counting from the moment the ",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext")," is created.\nNote that unlike in regular JavaScript, time is represented in seconds as a\nfloating point number."),(0,o.kt)("p",null,"We can't change ",(0,o.kt)("inlineCode",{parentName:"p"},"currentTime"),", we can only schedule things relative to it:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc = context.createOscillator()\n\nosc.connect(context.destination)\n\n// Start playing in 1 second\nosc.start(context.currentTime + 1.0)\n")),(0,o.kt)("div",{className:"admonition admonition-note alert alert--secondary"},(0,o.kt)("div",{parentName:"div",className:"admonition-heading"},(0,o.kt)("h5",{parentName:"div"},(0,o.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,o.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,o.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"}))),"IMG")),(0,o.kt)("div",{parentName:"div",className:"admonition-content"},(0,o.kt)("p",{parentName:"div"},"Timeline from init + relative timing"))),(0,o.kt)("p",null,"Beyond getting and scheduling relative to ",(0,o.kt)("inlineCode",{parentName:"p"},"currentTime"),", the Web Audio API gives\nus very little else to work with. Later in the book we'll create our own timing\nabstractions to handle beats, bars, and time signatures."),(0,o.kt)("h2",{id:"aside-autoplay-policy"},"Aside: Autoplay Policy"),(0,o.kt)("p",null,"Browsers generally require some interaction from the user before playing sound.\nThis\n",(0,o.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/Media/Autoplay_guide"},'"Autoplay Policy"'),"\ndiffers between browsers, but generally means we need to check for permission\nbefore we can use the Web Audio API."),(0,o.kt)("p",null,"To detect if we're allowed to play, we can check the ",(0,o.kt)("inlineCode",{parentName:"p"},"state")," of the\n",(0,o.kt)("inlineCode",{parentName:"p"},"AudioContext"),":"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconsole.log(context.state) // => running | suspended | closed\n")),(0,o.kt)("p",null,"If the ",(0,o.kt)("inlineCode",{parentName:"p"},"state")," is ",(0,o.kt)("inlineCode",{parentName:"p"},"running"),", it means we're allowed to play without any further\ninteraction from the user. If it's ",(0,o.kt)("inlineCode",{parentName:"p"},"suspended"),', we need to do a bit more work to\nprompt the user to interact with the page, usually in the form of clicking a\n"play" button:'),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconsole.log(context.state) // => suspended\n\ndocument.querySelector('#play').addEventListener('click', async () => {\n  await context.resume()\n  console.log(context.state) // => running\n})\n")),(0,o.kt)("p",null,"For development, it can be useful to always allow autoplay on specific domains\n(e.g. ",(0,o.kt)("inlineCode",{parentName:"p"},"localhost"),'). This can be configured in your browser settings (search for\n"sound" or "autoplay").'),(0,o.kt)("h2",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"As we've seen in this chapter, the Web Audio API (and audio programming in\ngeneral) introduces some concepts that may be new to web developers, namely the\naudio graph and a different timing model. We've also seen that the Web Audio API\nprovides only basic primitives for working with audio."),(0,o.kt)("p",null,"With this in mind, the next section of the book dives deep into building\nabstractions on top of the Web Audio API to express the building blocks of\nmusic."),(0,o.kt)("h2",{id:"further-reading"},"Further Reading"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/JavaScript"},"JavaScript")," \u2013\nMozilla Developer Network"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://eloquentjavascript.net/"},"Eloquent Javascript")," \u2013 Marijn Haverbeke"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.oreilly.com/library/view/javascript-the-definitive/9781491952016/"},"JavaScript: The Definitive Guide"),"\n\u2013 David Flanagan"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/getify/You-Dont-Know-JS"},"You Don't Know JS")," \u2013 Kyle Simpson"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://leanpub.com/javascriptallongesix"},"JavaScript Allong\xe9")," \u2013 Reg\nBraithwaite"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"http://teropa.info/blog/2016/08/19/what-is-the-web-audio-api.html"},"What Is the Web Audio API?"),"\n\u2013 Tero Parviainen"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://teropa.info/blog/2016/07/28/javascript-systems-music.html"},"JavaScript Systems Music"),"\n\u2013\xa0Tero Parviainen"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://medium.com/@metalex9/making-generative-music-in-the-browser-bfb552a26b0b"},"Making Generative Music in the Browser"),"\n\u2013\xa0Alex Bainter")))}m.isMDXComponent=!0},4508:function(e,t,n){t.Z=n.p+"assets/images/basic-graph-872901241f4c1a1eacb7761c560e7588.png"}}]);