"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[105],{3905:function(e,n,t){t.d(n,{Zo:function(){return u},kt:function(){return d}});var o=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function s(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?s(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):s(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},s=Object.keys(e);for(o=0;o<s.length;o++)t=s[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(o=0;o<s.length;o++)t=s[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=o.createContext({}),c=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},u=function(e){var n=c(e.components);return o.createElement(l.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},m=o.forwardRef((function(e,n){var t=e.components,a=e.mdxType,s=e.originalType,l=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),m=c(t),d=a,h=m["".concat(l,".").concat(d)]||m[d]||p[d]||s;return t?o.createElement(h,r(r({ref:n},u),{},{components:t})):o.createElement(h,r({ref:n},u))}));function d(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var s=t.length,r=new Array(s);r[0]=m;var i={};for(var l in n)hasOwnProperty.call(n,l)&&(i[l]=n[l]);i.originalType=e,i.mdxType="string"==typeof e?e:a,r[1]=i;for(var c=2;c<s;c++)r[c]=t[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}m.displayName="MDXCreateElement"},3088:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return i},contentTitle:function(){return l},metadata:function(){return c},toc:function(){return u},default:function(){return m}});var o=t(3117),a=t(102),s=(t(7294),t(3905)),r=["components"],i={title:"Notes"},l=void 0,c={unversionedId:"music/notes",id:"music/notes",title:"Notes",description:"A good place to start is to understand how to create a musical note. This guide",source:"@site/docs/music/notes.md",sourceDirName:"music",slug:"/music/notes",permalink:"/generative-music-with-javascript/music/notes",tags:[],version:"current",frontMatter:{title:"Notes"},sidebar:"main",previous:{title:"JavaScript",permalink:"/generative-music-with-javascript/primers/javascript"},next:{title:"Timing",permalink:"/generative-music-with-javascript/music/timing"}},u=[{value:"Synthesis",id:"synthesis",children:[],level:2},{value:"Sample",id:"sample",children:[],level:2},{value:"Learning",id:"learning",children:[],level:2},{value:"Playing Different Notes",id:"playing-different-notes",children:[{value:"Pitch Shifting",id:"pitch-shifting",children:[],level:3},{value:"Samplers and Sample Libraries",id:"samplers-and-sample-libraries",children:[],level:3},{value:"Sampler v1",id:"sampler-v1",children:[],level:3}],level:2},{value:"Mapping Notes to Samples",id:"mapping-notes-to-samples",children:[{value:"Enharmonic Notes",id:"enharmonic-notes",children:[],level:3},{value:"Sample Map",id:"sample-map",children:[],level:3},{value:"Note Numbers",id:"note-numbers",children:[],level:3},{value:"Sampler v2",id:"sampler-v2",children:[],level:3}],level:2},{value:"Controlling Notes",id:"controlling-notes",children:[{value:"Volume",id:"volume",children:[],level:3},{value:"Envelope",id:"envelope",children:[],level:3},{value:"Panning",id:"panning",children:[],level:3},{value:"Compression",id:"compression",children:[],level:3},{value:"Reverb",id:"reverb",children:[],level:3},{value:"Sampler v3",id:"sampler-v3",children:[],level:3}],level:2},{value:"Learning",id:"learning-1",children:[],level:2}],p={toc:u};function m(e){var n=e.components,i=(0,a.Z)(e,r);return(0,s.kt)("wrapper",(0,o.Z)({},p,i,{components:n,mdxType:"MDXLayout"}),(0,s.kt)("p",null,"A good place to start is to understand how to create a musical note. This guide\ncovers how to generate a single note, first using synthesis, then using a\nsample."),(0,s.kt)("h2",{id:"synthesis"},"Synthesis"),(0,s.kt)("p",null,"The Web Audio API provides all the building blocks required to synthesise your\nown sounds by combining oscillators and effects. The easiest way to hear some\nnoise is to create an\n",(0,s.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode"},(0,s.kt)("inlineCode",{parentName:"a"},"OscillatorNode")),"."),(0,s.kt)("p",null,"An oscillator can be tuned to (or oscillate at) a specific frequency. Musical\nnotes are just the names we've given to certain frequencies. If we know the\nfrequency of a note, we can set the oscillator to that frequency and it will\nproduce that note."),(0,s.kt)("p",null,"Here, we set the oscillator to 440 Hz, which is the frequency of the middle C\n(C4) note on a piano. The default oscillator type is ",(0,s.kt)("inlineCode",{parentName:"p"},"sine"),", so when running\nthis code we hear a pure sine wave tone at middle C."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc = context.createOscillator()\nosc.frequency.value = 440\nosc.connect(context.destination)\nosc.start()\n")),(0,s.kt)("p",null,"With an understanding of sound synthesis you can generate an infinite range of\nsounds. As mentioned in the introduction, however, it's not the focus of this\nbook, so we won't delve much deeper into synthesis here."),(0,s.kt)("h2",{id:"sample"},"Sample"),(0,s.kt)("p",null,"Our focus is music composition, so rather than synthesising our own sounds,\nwe're instead going to delegate that work to pre-recorded instrument samples."),(0,s.kt)("p",null,"To play back samples we need a few elements:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"A sample audio file: e.g.\n",(0,s.kt)("a",{target:"_blank",href:t(9542).Z},"the middle C (C4) note played on a piano")),(0,s.kt)("li",{parentName:"ul"},"A way to load the audio file:\n",(0,s.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API"},(0,s.kt)("inlineCode",{parentName:"a"},"fetch"))),(0,s.kt)("li",{parentName:"ul"},"A way to decode the audio file for playback:\n",(0,s.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/BaseAudioContext/decodeAudioData"},(0,s.kt)("inlineCode",{parentName:"a"},"decodeAudioData"))),(0,s.kt)("li",{parentName:"ul"},"A place to store the decoded audio:\n",(0,s.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer"},(0,s.kt)("inlineCode",{parentName:"a"},"AudioBuffer"))),(0,s.kt)("li",{parentName:"ul"},"A way to play back the decoded audio:\n",(0,s.kt)("a",{parentName:"li",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode"},(0,s.kt)("inlineCode",{parentName:"a"},"AudioBufferSourceNode")))),(0,s.kt)("p",null,"With these, we can create something equivalent to the synthesised example above,\nbut this time using our pre-recorded sample. When running this code, we hear our\npiano sample play from start to finish."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"// Create the Web Audio environment.\nconst context = new AudioContext()\n\n// Load a sample from the server.\nfetch('sample.mp3')\n  .then((response) => {\n    // Get the `arrayBuffer` representation of the sample.\n    return response.arrayBuffer()\n  })\n  .then((arrayBuffer) => {\n    // Decode the `arrayBuffer` into actual audio.\n    return context.decodeAudioData(arrayBuffer)\n  })\n  .then((audioBuffer) => {\n    // Create an `AudioBufferSourceNode`.\n    const sourceNode = context.createBufferSource()\n\n    // Assign the audio to its buffer.\n    sourceNode.buffer = audioBuffer\n\n    // Connect the `sourceNode` to the destination output (our speakers).\n    sourceNode.connect(context.destination)\n\n    // Start playback of the sample.\n    sourceNode.start()\n  })\n")),(0,s.kt)("h2",{id:"learning"},"Learning"),(0,s.kt)("p",null,"We now know the steps involved in loading a sample, decoding it, and playing it\nback. ",(0,s.kt)("strong",{parentName:"p"},"TODO")," includes the ",(0,s.kt)("inlineCode",{parentName:"p"},"sample()")," function which abstracts away some of\nthese details for us:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const { sample } = tuplet\n\n;(async () => {\n  const context = new AudioContext()\n  const s = await sample(context, 'sample.mp3')\n  s.connect(context.destination)\n  s.start()\n})()\n")),(0,s.kt)("h2",{id:"playing-different-notes"},"Playing Different Notes"),(0,s.kt)("p",null,"Our musical options would be quite limited if we could only play middle C, so\nlet's explore ways to make other notes."),(0,s.kt)("h3",{id:"pitch-shifting"},"Pitch Shifting"),(0,s.kt)("p",null,"One approach we could take is to adjust the pitch of our sample to emulate\ndifferent notes. Thanks to the intricacies of frequency and the human ear, we\nperceive a sample played at twice the speed to be twice the pitch."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\n\n// Half speed = 22,050 Hz = -1 octave = C3\nconst c3 = context.createBufferSource()\nc3.buffer = audioBuffer\nc3.playbackRate.value = 0.5\n\n// Full speed = 44,100 Hz = C4\nconst c4 = context.createBufferSource()\nc4.buffer = audioBuffer\nc4.playbackRate.value = 1.0\n\n// Double speed = 88,200 Hz = +1 octave = C5\nconst c5 = context.createBufferSource()\nc5.buffer = audioBuffer\nc5.playbackRate.value = 2.0\n")),(0,s.kt)("p",null,"This works surprisingly well within a limited range, but pitch shifting too much\ncan start to sound unnatural, especially for real instruments, where we have an\nexpectation of how they should sound at different pitches."),(0,s.kt)("h3",{id:"samplers-and-sample-libraries"},"Samplers and Sample Libraries"),(0,s.kt)("p",null,"A better approach, and one used in most music production, is to have a sample\nfor each note. This brings us to the topic of samplers and sample libraries."),(0,s.kt)("p",null,"A sampler, at its most basic, is an instrument that can load and play back\nsamples. You can assign samples to physical triggers such as the keys of a\nkeyboard, so that when pressing the key, the assigned sample is played."),(0,s.kt)("p",null,"Sample libraries (or sample packs) are bundles of samples, usually with a\nmetadata file that tells the sampler how the individual sample files map to\nkeyboard notes. A library maker records each individual note of an instrument,\noften at various velocities (e.g. how hard a piano key is hit) and with\ndifferent techniques (e.g. whether a violin string is bowed or plucked) to\ncapture the full range of sounds the instrument can make. Whilst this can't\nmatch the subtleties of how a musician might play, it's usually good enough for\ncomposition."),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("strong",{parentName:"p"},"Note:")," Sample libraries don't always contain recordings of all possible\nnotes, but combine a subset of samples with the pitch shifting trick mentioned\nabove.")),(0,s.kt)("p",null,"Most sample libraries are commercial products, often bundled with software\nsamplers, but there are several libraries available for free use. ",(0,s.kt)("strong",{parentName:"p"},"Tuplet"),"\nincludes a range of samples for different instruments, which we'll be using\nthroughout this book. It includes a\n",(0,s.kt)("a",{parentName:"p",href:"https://github.com/meleyal/tuplet/tree/master/src/samples/piano"},"piano")," sample\npack, which contains recordings of all the notes of a piano (A0\u2013C8)."),(0,s.kt)("h3",{id:"sampler-v1"},"Sampler v1"),(0,s.kt)("p",null,"We've already seen how we can load and play back a single sample. We can use the\nsame principles to load and play back an entire set of samples."),(0,s.kt)("p",null,"Let's create a ",(0,s.kt)("inlineCode",{parentName:"p"},"sampler()")," function that takes an audio context and a map of\nsamples we want to load, and returns a function for playing them back:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const sampler = async (context, samples) => {\n  const buffers = await Promise.all(\n    Object.keys(samples).map((note) =>\n      fetch(samples[note])\n        .then((response) => response.arrayBuffer())\n        .then((arrayBuffer) => context.decodeAudioData(arrayBuffer))\n        .then((buffer) => Object.create({ note, buffer }))\n    )\n  )\n\n  return (note) => {\n    const notes = typeof note == 'string' ? [note] : note\n    const now = context.currentTime\n    notes.map((n) => {\n      const buffer = buffers.find((b) => b.note == n).buffer\n      const sourceNode = context.createBufferSource()\n      sourceNode.buffer = buffer\n      sourceNode.start(now)\n      sourceNode.connect(context.destination)\n    })\n  }\n}\n;(async () => {\n  const context = new AudioContext()\n\n  const piano = await sampler(context, {\n    C4: 'samples/piano/c4.mp3',\n    D4: 'samples/piano/d4.mp3',\n    E4: 'samples/piano/e4.mp3',\n    F4: 'samples/piano/f4.mp3',\n    G4: 'samples/piano/g4.mp3',\n    // ...etc.\n  })\n\n  // Single C note\n  piano('C4')\n\n  // C major chord\n  piano(['C4', 'E4', 'G4'])\n})()\n")),(0,s.kt)("h2",{id:"mapping-notes-to-samples"},"Mapping Notes to Samples"),(0,s.kt)("p",null,"Rather than writing out a sample map of 88 notes by hand, we can automate this\nprocess with a ",(0,s.kt)("inlineCode",{parentName:"p"},"sampleMap()")," function. But first, we'll need to understand a\nsmall nuance of sample libraries."),(0,s.kt)("h3",{id:"enharmonic-notes"},"Enharmonic Notes"),(0,s.kt)("p",null,"If you inspect the piano samples, you might notice that they don't seem to\nactually include ",(0,s.kt)("em",{parentName:"p"},"all")," of the notes we need:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-text"},".\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 c4.mp3\n\u251c\u2500\u2500 ... <- where is c#4.mp3?\n\u251c\u2500\u2500 db4.mp3\n\u251c\u2500\u2500 d4.mp3\n\u251c\u2500\u2500 ...\n")),(0,s.kt)("p",null,"Recall from the ",(0,s.kt)("a",{parentName:"p",href:"../primers/music"},"Music")," chapter that some notes can be sharp\n(C#, half a semitone above C) or flat (Db, half a semitone below D). Looking at\nour keyboard, we can see that these are actually the same note:"),(0,s.kt)("p",null,(0,s.kt)("img",{src:t(3503).Z,width:"334",height:"230"})),(0,s.kt)("p",null,'These are known as "enharmonic" notes, which just means they are the same note\nwritten in a different way. We can write a simple ',(0,s.kt)("inlineCode",{parentName:"p"},"enharmonic()")," function to\nconvert in either direction:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const enharmonic = (note) => {\n  switch (note) {\n    case 'A#':\n      return 'Bb'\n    case 'Bb':\n      return 'A#'\n    case 'C#':\n      return 'Db'\n    case 'Db':\n      return 'C#'\n    case 'D#':\n      return 'Eb'\n    case 'Eb':\n      return 'D#'\n    case 'F#':\n      return 'Gb'\n    case 'Gb':\n      return 'F#'\n    case 'G#':\n      return 'Ab'\n    case 'Ab':\n      return 'G#'\n    default:\n      return note\n  }\n}\n\nenharmonic('C#') // => Db\nenharmonic('Db') // => C#\n")),(0,s.kt)("h3",{id:"sample-map"},"Sample Map"),(0,s.kt)("p",null,"With our knowledge of enharmonics in hand, let's write a ",(0,s.kt)("inlineCode",{parentName:"p"},"sampleMap()")," function\nthat maps the 88 note names to their respective sample files, taking into\naccount enharmonic naming (e.g. ",(0,s.kt)("inlineCode",{parentName:"p"},"C#4")," maps to ",(0,s.kt)("inlineCode",{parentName:"p"},"db4.mp3"),")."),(0,s.kt)("p",null,"Different sample libraries may use different naming conventions and file\nformats, so to account for that, our function will take a ",(0,s.kt)("inlineCode",{parentName:"p"},"pathFn")," function to\nconstruct the final url to the sample."),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},(0,s.kt)("strong",{parentName:"p"},"Note:")," Here we're mapping over all octaves and notes, but most instruments\nhave a more limited range than the piano. A fully-fledged ",(0,s.kt)("inlineCode",{parentName:"p"},"sampleMap()"),"\nimplementation might allow for specifying a given range to map over.")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},'const { enharmonic } = tuplet\n\nconst sampleMap = (pathFn) => {\n  const notes = \'C,C#,D,D#,E,F,F#,G,G#,A,A#,B\'.split(\',\')\n  const octaves = [1, 2, 3, 4, 5, 6, 7]\n\n  return notes\n    .flatMap((note) =>\n      octaves.map((octave) => {\n        const name = `${note}${octave}`\n        return { [name]: pathFn(note, octave) }\n      })\n    )\n    .reduce((a, b) => Object.assign(a, b), {})\n}\n\nconst samples = sampleMap((note, octave) => {\n  const baseUrl = \'https://example.com\'\n  const noteName = enharmonic(note).toLowerCase()\n  return `${baseUrl}/samples/piano/${noteName}${octave}.mp3`\n})\n\nconsole.log(samples)\n\n// Logs:\n//\n//  {\n//    A1: "http://localhost:3001/samples/piano/a1.mp3",\n//    A2: "http://localhost:3001/samples/piano/a2.mp3",\n//    A3: "http://localhost:3001/samples/piano/a3.mp3",\n//    A4: "http://localhost:3001/samples/piano/a4.mp3",\n//    A5: "http://localhost:3001/samples/piano/a5.mp3",\n//    ...etc.\n//  }\n')),(0,s.kt)("h3",{id:"note-numbers"},"Note Numbers"),(0,s.kt)("p",null,"When we get into generating notes and patterns later in the book, it will be\nuseful to be able to translate between note names and numbers. MIDI, which we\ncovered briefly in the ",(0,s.kt)("a",{parentName:"p",href:"../primers/music"},"Music")," chapter, already has a\nconvention for note numbers, so we'll use that."),(0,s.kt)("p",null,"As we know, a piano keyboard has 88 keys spanning 7 octaves, going from A0 to\nC8. MIDI note numbers go from 0 (C-1) to 127 (G9), which encompasses the full\nrange of notes produced by\n",(0,s.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Range_(music)"},"most instruments"),"."),(0,s.kt)("p",null,"Let's write two functions ",(0,s.kt)("inlineCode",{parentName:"p"},"noteNumber()")," to get the MIDI note number given a\nnote name, and ",(0,s.kt)("inlineCode",{parentName:"p"},"noteName()")," to get the the note name from the MIDI note number."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const noteNumber = (name) => {\n  const re = /(?<note>\\w(\\w|\\W)?)(?<octave>\\d{1})/u\n  const {\n    groups: { note, octave },\n  } = re.exec(name)\n\n  const notes = {\n    C: 0,\n    'C#': 1,\n    Db: 1,\n    D: 2,\n    'D#': 3,\n    Eb: 3,\n    E: 4,\n    F: 5,\n    'F#': 6,\n    Gb: 6,\n    G: 7,\n    'G#': 8,\n    Ab: 8,\n    A: 9,\n    'A#': 10,\n    Bb: 10,\n    B: 11,\n  }\n\n  return notes[note] + 12 + 12 * octave\n}\n\nnoteNumber('C0') // => 12\nnoteNumber('C4') // => 60\nnoteNumber('Gb4') // => 66\nnoteNumber('G4') // => 67\nnoteNumber('A4') // => 69\nnoteNumber('A#4') // => 70\nnoteNumber('Bb4') // => 70\nnoteNumber('B4') // => 71\n")),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const noteName = (num) => {\n  const numbers = {\n    0: 'C',\n    1: 'C#/Db',\n    2: 'D',\n    3: 'D#/Eb',\n    4: 'E',\n    5: 'F',\n    6: 'F#/Gb',\n    7: 'G',\n    8: 'G#/Ab',\n    9: 'A',\n    10: 'A#/Bb',\n    11: 'B',\n  }\n\n  // Normalize the note number so it maps to our 0-indexed `numbers` map.\n  const norm = num - 12\n\n  // Dividing the note number by 12 (the number of notes in an octave) gives us\n  // the octave that the note falls into.\n  const octave = Math.floor(norm / 12)\n\n  // Remove the octaves to get a valid index into our numbers map.\n  const note = norm - 12 * octave\n\n  return numbers[note]\n    .split('/')\n    .map((name) => name + octave)\n    .join('/')\n}\n\nnoteName(12) // => C0\nnoteName(14) // => D0\nnoteName(21) // => A0\nnoteName(24) // => C1\nnoteName(60) // => C4\nnoteName(80) // => G#5/Ab5\nnoteName(107) // => B7\n")),(0,s.kt)("h3",{id:"sampler-v2"},"Sampler v2"),(0,s.kt)("p",null,"Putting this all together, we can combine our ",(0,s.kt)("inlineCode",{parentName:"p"},"sampleMap()")," function with a\nmodified version of ",(0,s.kt)("inlineCode",{parentName:"p"},"sampler()")," that can play notes given either a note name or\nnote number:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const { noteName, enharmonic, sampleMap } = tuplet\n\nconst sampler = async (context, samples) => {\n  const buffers = await Promise.all(\n    Object.keys(samples).map((note) =>\n      fetch(samples[note])\n        .then((response) => response.arrayBuffer())\n        .then((arrayBuffer) => context.decodeAudioData(arrayBuffer))\n        .then((buffer) => Object.create({ note, buffer }))\n    )\n  )\n\n  const parseNote = (note) => {\n    if (Array.isArray(note)) {\n      return note.map(parseNote)\n    } else if (typeof note === 'number') {\n      return [noteName(note)]\n    } else {\n      return [note]\n    }\n  }\n\n  return (note) => {\n    const notes = parseNote(note)\n    const now = context.currentTime\n    notes.map((n) => {\n      const buffer = buffers.find((b) => b.note == n).buffer\n      const sourceNode = context.createBufferSource()\n      sourceNode.buffer = buffer\n      sourceNode.start(now)\n      sourceNode.connect(context.destination)\n    })\n  }\n}\n\n;(async () => {\n  const context = new AudioContext()\n\n  const samples = sampleMap((note, octave) => {\n    const baseUrl = 'https://example.com'\n    const noteName = enharmonic(note).toLowerCase()\n    return `${baseUrl}/samples/piano/${noteName}${octave}.mp3`\n  })\n\n  const piano = await sampler(context, samples)\n\n  // Single C note\n  piano('C4')\n\n  // C major chord\n  piano(['C4', 'E4', 'G4'])\n\n  // Single C note\n  piano(60)\n\n  // C major chord\n  piano([60, 64, 67])\n})()\n")),(0,s.kt)("h2",{id:"controlling-notes"},"Controlling Notes"),(0,s.kt)("p",null,"This is a good start, but our sampler is still missing a few essential features."),(0,s.kt)("p",null,"Currently, once triggered, our notes play at full volume from start to finish.\nThis is equivalent to playing a piano and pressing each key with the same\nvelocity and holding it for the same length of time."),(0,s.kt)("p",null,"To model how a real piano is played, where notes may be quiet, loud, short, or\nlong, we need a way to control our samples as they are playing."),(0,s.kt)("h3",{id:"volume"},"Volume"),(0,s.kt)("p",null,"We can control the volume of a sample with a\n",(0,s.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/GainNode"},(0,s.kt)("inlineCode",{parentName:"a"},"GainNode")),", which\nchanges the volume of any signal passing through it according to its\n",(0,s.kt)("inlineCode",{parentName:"p"},"gain.value"),"."),(0,s.kt)("blockquote",null,(0,s.kt)("p",{parentName:"blockquote"},"We use gain and volume interchangeably here, but technically ",(0,s.kt)("em",{parentName:"p"},"gain")," is the\nchange applied, resulting in a different ",(0,s.kt)("em",{parentName:"p"},"volume"),".")),(0,s.kt)("p",null,"Returning back to our simple oscillator example, we can control its volume by\ninserting a ",(0,s.kt)("inlineCode",{parentName:"p"},"GainNode")," between it and our speakers."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc = context.createOscillator()\nconst volume = context.createGain()\n\nosc.connect(volume)\nvolume.connect(context.destination)\n\nvolume.gain.value = 0.5\nosc.start()\n")),(0,s.kt)("h3",{id:"envelope"},"Envelope"),(0,s.kt)("p",null,"To control how long a note lasts, we can use what's known as an ",(0,s.kt)("em",{parentName:"p"},"envelope"),". An\nenvelope controls how a sound evolves over time, and is broken down into four\nphases:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"Attack: when a note is triggered, how long it takes to reach full volume."),(0,s.kt)("li",{parentName:"ul"},"Decay: how long it takes to drop to the sustain level."),(0,s.kt)("li",{parentName:"ul"},"Sustain: the constant volume after decay until a note is released."),(0,s.kt)("li",{parentName:"ul"},"Release: how quickly the sound fades after a note is released.")),(0,s.kt)("p",null,"These four phases define what's known as an ",(0,s.kt)("em",{parentName:"p"},"ADSR envelope"),". For our purposes,\nwe can simplify this down to just the attack and release phases, which will\nallow us to control how a sound peaks, and how long it lasts, known as an ",(0,s.kt)("em",{parentName:"p"},"AR\nenvelope"),":"),(0,s.kt)("p",null,(0,s.kt)("img",{src:t(9312).Z,width:"762",height:"229"})),(0,s.kt)("p",null,"Envelopes can be modelled with a ",(0,s.kt)("inlineCode",{parentName:"p"},"GainNode"),", taking advantage of the fact that\nits ",(0,s.kt)("inlineCode",{parentName:"p"},"gain")," property is an\n",(0,s.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/AudioParam"},(0,s.kt)("inlineCode",{parentName:"a"},"AudioParam"))," that\ncan be controlled over time:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc = context.createOscillator()\nconst envelope = context.createGain()\n\nconst now = context.currentTime\nconst zero = 0.00001 // value must be positive for exponentialRamp\nconst volume = 1 // full volume\nconst attack = 1 // note takes 1 second to reach full volume\nconst release = 3 // note lasts for 3 seconds\n\nenvelope.gain\n  .setValueAtTime(0, now)\n  .linearRampToValueAtTime(volume, now + attack)\n  .exponentialRampToValueAtTime(zero, now + attack + release)\n\nosc.connect(envelope)\nenvelope.connect(context.destination)\nosc.start()\n")),(0,s.kt)("h3",{id:"panning"},"Panning"),(0,s.kt)("p",null,"Panning describes where a sound is placed in the stereo field, and emulates the\neffect of sounds coming from different physical spaces. When mixing different\nsounds together, panning can give each sound it's own place in the mix."),(0,s.kt)("p",null,'For our purposes, we\'re only working with two audio channels: left and right.\nPanning essentially just controls how "much" of a sound goes to each channel.'),(0,s.kt)("p",null,"To pan sounds we can use a\n",(0,s.kt)("a",{parentName:"p",href:"https://developer.mozilla.org/en-US/docs/Web/API/StereoPannerNode"},(0,s.kt)("inlineCode",{parentName:"a"},"StereoPannerNode")),".\nSetting its ",(0,s.kt)("inlineCode",{parentName:"p"},"pan.value")," determines where the sound is panned, ranging from ",(0,s.kt)("inlineCode",{parentName:"p"},"-1"),"\nbeing fully left, ",(0,s.kt)("inlineCode",{parentName:"p"},"0")," being centre, to ",(0,s.kt)("inlineCode",{parentName:"p"},"1")," being fully right."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc = context.createOscillator()\nconst panner = context.createStereoPanner()\n\npanner.pan.value = -1 // fully left\n\nosc.connect(panner)\npanner.connect(context.destination)\nosc.start()\n")),(0,s.kt)("h3",{id:"compression"},"Compression"),(0,s.kt)("p",null,"Compression, in simple terms, is the process of modifying, or ",(0,s.kt)("em",{parentName:"p"},"limiting"),", the\nquietness and/or loudness of a sound signal."),(0,s.kt)("p",null,"When combining multiple sounds, their volumes (or ",(0,s.kt)("em",{parentName:"p"},"amplitudes"),") are added\ntogether, which can result in distortion if their combined amplitudes exceed the\nrange of our speakers. By running our audio signal through a compressor, we can\nensure that the resulting signal never exceeds or drops below a given threshold."),(0,s.kt)("p",null,"The example below shows combining two oscillators without compression. If you\nturn up your volume, you should notice that the sound starts to distort at some\npoint:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst osc1 = context.createOscillator()\nconst osc2 = context.createOscillator()\n\nosc1.frequency.value = 440\nosc2.frequency.value = 880\n\nosc1.connect(context.destination)\nosc2.connect(context.destination)\n\nosc1.start()\nosc2.start()\n")),(0,s.kt)("p",null,"Compare this with the example below where we run both oscillators through a\ncompressor. Even when increasing the volume to maximum, there should be no\ndistortion:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const context = new AudioContext()\nconst now = context.currentTime\n\nconst osc1 = context.createOscillator()\nconst osc2 = context.createOscillator()\nconst compressor = context.createDynamicsCompressor()\n\nosc1.frequency.value = 440\nosc2.frequency.value = 880\n\ncompressor.threshold.setValueAtTime(-50, now)\ncompressor.knee.setValueAtTime(40, now)\ncompressor.ratio.setValueAtTime(12, now)\ncompressor.attack.setValueAtTime(0, now)\ncompressor.release.setValueAtTime(0.25, now)\n\nosc1.connect(compressor)\nosc2.connect(compressor)\ncompressor.connect(context.destination)\n\nosc1.start()\nosc2.start()\n")),(0,s.kt)("h3",{id:"reverb"},"Reverb"),(0,s.kt)("p",null,"TODO: Impulse responses."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const reverbNode = await createReverb(context, context.destination, reverb)\n\nconst createReverb = async (context, output, impulse) => {\n  const impulseBuffer = await loadImpulse(context, reverbSamples[impulse])\n  const convolverNode = context.createConvolver()\n  convolverNode.buffer = impulseBuffer.buffer\n  convolverNode.connect(output)\n  return convolverNode\n}\n")),(0,s.kt)("h3",{id:"sampler-v3"},"Sampler v3"),(0,s.kt)("p",null,"Putting all of these concepts together, we can build a sampler."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre",className:"language-js"},"const sampler = async (context, samples) => {\n  const buffers = await Promise.all(\n    Object.keys(samples).map((note) =>\n      fetch(samples[note])\n        .then((response) => response.arrayBuffer())\n        .then((arrayBuffer) => context.decodeAudioData(arrayBuffer))\n        .then((buffer) => Object.create({ note, buffer }))\n    )\n  )\n\n  const compressorNode = context.createDynamicsCompressor()\n  compressorNode.threshold.value = -50\n  compressorNode.knee.value = 40\n  compressorNode.ratio.value = 12\n  compressorNode.attack.value = 0\n  compressorNode.release.value = 0.25\n\n  const gainNode = context.createGain()\n\n  return (note, options) => {\n    const now = context.currentTime\n    const notes = typeof note == 'string' ? [note] : note\n    const defaults = { volume: 1, duration: Infinity }\n    const { volume, duration } = Object.assign(defaults, options)\n\n    notes.map((n) => {\n      const buffer = buffers.find((b) => b.note == n).buffer\n      const sourceNode = context.createBufferSource()\n      sourceNode.buffer = buffer\n      sourceNode.start()\n\n      const zero = 0.00001 // value must be positive for exponentialRamp\n      gainNode.gain.value = volume\n      gainNode.gain.exponentialRampToValueAtTime(\n        zero,\n        now + Math.min(duration, buffer.duration)\n      )\n\n      sourceNode.connect(gainNode)\n      gainNode.connect(compressorNode)\n      compressorNode.connect(context.destination)\n    })\n  }\n}\n\ntuplet.run(async (context) => {\n  const piano = await sampler(context, tuplet.sampleMap('samples/piano/'))\n\n  // Single C note\n  piano('C4', { volume: 0.5 })\n\n  // C major chord\n  piano(['C4', 'E4', 'G4'], { volume: 0.8, duration: 2 })\n})\n")),(0,s.kt)("h2",{id:"learning-1"},"Learning"),(0,s.kt)("p",null,"TODO: Mention that these functions are part of Tuplet, and we'll use them going\nforward."))}m.isMDXComponent=!0},9542:function(e,n,t){n.Z=t.p+"assets/files/piano-c4-205f9346db19cb55917be4cfa717a3c4.mp3"},9312:function(e,n,t){n.Z=t.p+"assets/images/envelopes-39b756f34a94d6e2ddaa483c71600797.svg"},3503:function(e,n){n.Z="data:image/svg+xml;base64,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"}}]);